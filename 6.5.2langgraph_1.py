from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from tavily import TavilyClient
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver



load_dotenv()


class SearchState(TypedDict):
    messages: Annotated[list, add_messages]
    user_query: str         # ç»è¿‡ LLM ç†è§£åçš„ç”¨æˆ·éœ€æ±‚æ€»ç»“
    search_query: str       # ä¼˜åŒ–åç”¨äº Tavily Api çš„æœç´¢æŸ¥è¯¢
    search_result: str      # Tavily æœç´¢è¿”å›çš„ç»“æœ
    final_answer: str       # æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ
    step: str               # æ ‡è®°å½“å‰æ­¥éª¤


# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(
    model=os.getenv("LLM_MODEL_ID"),
    api_key=os.getenv("LLM_API_KEY"),
    base_url=os.getenv("LLM_BASE_URL"),
    temperature=0.7
)

# åˆå§‹åŒ–Taviltå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))


# --- å®šä¹‰èŠ‚ç‚¹ ---
def understand_query_node(state: SearchState) -> dict:
    """æ­¥éª¤1ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯"""
    user_message = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break

    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"
è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""
    
    response = llm.invoke([SystemMessage(content=understand_prompt)])

    # æå–æœç´¢å…³é”®è¯
    response_text = response.content
    # è§£æ LLM çš„è¾“å‡ºï¼Œæå–æœç´¢å…³é”®è¯
    serach_query = user_message # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢

    if "æœç´¢è¯ï¼š" in response_text:
        serach_query = response_text.split("æœç´¢è¯ï¼š")[1].strip()
    elif "æœç´¢å…³é”®è¯ï¼š" in response_text:
        serach_query = response_text.split("æœç´¢å…³é”®è¯ï¼š")[1].strip()
    
    return {
        "user_query": response_text,
        "search_query": serach_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘å°†ä¸ºä½ æœç´¢ï¼š{serach_query}")]
    }

def tavily_search_node(state: SearchState) -> dict:
    """æ­¥éª¤2ï¼šä½¿ç”¨ Tavily API è¿›è¡Œæœç´¢"""
    search_query = state["search_query"]
    
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")

        response = tavily_client.search(
            query=search_query, 
            search_depth="basic",
            include_answer=True,
            include_raw_content=False,
            max_results=5
        )

        # å¤„ç†æœç´¢ç»“æœ
        search_results = ""

        # ä¼˜å…ˆä½¿ç”¨ Tavily çš„ç»¼åˆç­”æ¡ˆ
        if response.get("answer"):
            search_results = f"ç»¼åˆç­”æ¡ˆï¼š\n{response["answer"]}\n\n"

        # æ·»åŠ å…·ä½“çš„æœç´¢ç»“æœ
        if response.get("results"):
            search_results += "ç›¸å…³ä¿¡æ¯ï¼š\n"
            for i, result in enumerate(response["results"][:3], 1):
                title = result.get("title", "")
                content = result.get("content", "")
                url = result.get("url", "")
                search_results += f"{i}. {title}\n{content}\næ¥æº: {url}\n\n"
        
        if not search_results:
            search_results = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚ "
        
        return {
            "search_result": search_results,
            "step": "searched",
            "messages": [AIMessage(content=f"âœ… æœç´¢å®Œæˆï¼æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ï¼Œæ­£åœ¨ä¸ºæ‚¨æ•´ç†ç­”æ¡ˆ...")]
        }
    except Exception as e:
        error_msg = f"æœç´¢æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "search_result": f"æœç´¢å¤±è´¥ï¼š{error_msg}",
            "step": "search_field",
            "messages": [AIMessage(content=f"âŒ æœç´¢é‡åˆ°é—®é¢˜ï¼Œæˆ‘å°†åŸºäºå·²æœ‰çŸ¥è¯†ä¸ºæ‚¨å›ç­”ã€‚")]
        }
    
def generate_answer_node(state: SearchState) -> dict:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    if state["step"] == "search_field":
        # å¦‚æœæœç´¢å¤±è´¥ï¼Œæ‰§è¡Œå›é€€ç­–ç•¥ï¼ŒåŸºäº LLM è‡ªèº«çŸ¥è¯†å›ç­”
        fallback_prompt = f"""æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜:
ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

è¯·æä¾›ä¸€ä¸ªæœ‰ç”¨çš„å›ç­”ï¼Œå¹¶è¯´æ˜åŸºäºå·²æœ‰çŸ¥è¯†çš„å›ç­”ã€‚"""
        response = llm.invoke([SystemMessage(content=fallback_prompt)])
    else:
        # æœç´¢æˆåŠŸï¼ŒåŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        answer_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š

ç”¨æˆ·é—®é¢˜: {state['user_query']}

æœç´¢ç»“æœï¼š\n{state['search_result']}

è¯·è¦æ±‚ï¼š
1. ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”
2. å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆæˆ–ä»£ç 
3. å¼•ç”¨é‡è¦ä¿¡æ¯çš„æ¥æº
4. å›ç­”è¦ç»“æ„æ¸…æ™°ã€æ˜“äºç†è§£
5. å¦‚æœæœç´¢ç»“æœä¸å¤Ÿå®Œæ•´ï¼Œè¯·è¯´æ˜å¹¶æä¾›è¡¥å……å»ºè®®"""
        
        response = llm.invoke([SystemMessage(content=answer_prompt)])
    
    return {
        "final_answer": response.content,
        "step": "completed",
        "messages": [AIMessage(content=response.content)]
    }

# --- æ„å»ºçŠ¶æ€å›¾ ---

def create_search_assistant():
    workflow = StateGraph(SearchState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("ubderstand", understand_query_node)
    workflow.add_node("search", tavily_search_node)
    workflow.add_node("answer", generate_answer_node)

    # è®¾ç½®çº¿æ€§æµç¨‹
    workflow.add_edge(START, "ubderstand")
    workflow.add_edge("ubderstand", "search")
    workflow.add_edge("search", "answer")
    workflow.add_edge("answer", END)

    # ç¼–è¯‘å›¾
    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memoryjinti)
    return app


async def main():
    if not os.getenv("TAVILY_API_KEY"):
        print("âŒ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® TAVILY_API_KEY ç¯å¢ƒå˜é‡ï¼")
        return
    
    app = create_search_assistant()

    print("ğŸ” æ™ºèƒ½æœç´¢åŠ©æ‰‹å¯åŠ¨ï¼")
    print("æˆ‘ä¼šä½¿ç”¨Tavily APIä¸ºæ‚¨æœç´¢æœ€æ–°ã€æœ€å‡†ç¡®çš„ä¿¡æ¯")
    print("æ”¯æŒå„ç§é—®é¢˜ï¼šæ–°é—»ã€æŠ€æœ¯ã€çŸ¥è¯†é—®ç­”ç­‰")
    print("è¾“å…¥ 'quit' é€€å‡º\m")

    session_count = 0

    while True:
        user_input = input("ğŸ¤” æ‚¨æƒ³äº†è§£ä»€ä¹ˆ: ").strip()

        if user_input.lower() in ["quit", "exit", "q", "é€€å‡º"]:
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼å†è§ï¼")
            break

        if not user_input:
            continue

        session_count += 1
        config = {"configurable": {"thread_id": f"search_session_{session_count}"}}

        # åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_query": "",
            "search_query": "",
            "search_result": "",
            "final_answer": "",
            "step": "start"
        }

        try:
            print("\nâ³ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨å€™...\n")

            async for output in app.astream(initial_state, config=config):
                for node_name, node_output in output.items():
                    if "messages" in node_output and node_output["messages"]:
                        latest_message = node_output["messages"][-1]
                        if isinstance(latest_message, AIMessage):
                            if node_name == "understand":
                                print(f"ğŸ§  ç†è§£é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "search":
                                print(f"ğŸ” æœç´¢é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "answer":
                                print(f"\nğŸ’¡ æœ€ç»ˆå›ç­”:\n{latest_message.content}")
            
            print("\nâœ… è¯·æ±‚å¤„ç†å®Œæ¯•ï¼\n")
        
        except Exception as e:
            print(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}\n")
            print("è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚\n")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
            

